{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6c100f1-96af-470b-b190-7ed8cd0fc1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class  BaseModel():\n",
    "    def __init__(self, model_name = ''):\n",
    "        \n",
    "        self.model =  ort.InferenceSession(model_name)\n",
    "    def preprocess(self, input):\n",
    "        return input\n",
    "        \n",
    "    def run(self, inputs):\n",
    "        return self.model.run(output_names=[ \"output\" ], input_feed=dict(inputs))\n",
    "\n",
    "    def postprocess(self, output):\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17346e07-e69a-452c-8907-11095b3ac412",
   "metadata": {},
   "source": [
    "# Речь в текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10a668ac-1ed8-42fd-96e6-32224e0865be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You are using a model of type whisper to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Слушай, я потратил обретьом кучу денег для того, чтобы притоситься в эту дару. Это что вообще такое? Посмотри на официантов, они все в черных каких-то рубашках с кислыми минами. Даже никто из них до сих пор не подошел к нам.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForQuestionAnswering, ORTModelForAudioClassification, ORTModelForSequenceClassification, ORTModelForSpeechSeq2Seq\n",
    "from transformers import AutoConfig, Wav2Vec2Processor, AutoTokenizer, PretrainedConfig, WhisperProcessor\n",
    "import time\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class  SpeechtoText(BaseModel):\n",
    "    def __init__(self, model_path, model_name, model_name_preprocess, postprocess = None):\n",
    "        self.processor =  WhisperProcessor.from_pretrained(model_name_preprocess)#\"openai/whisper-tiny\")\n",
    "        # self.sampling_rate = processor.feature_extractor.sampling_rate\n",
    "        model_config = PretrainedConfig.from_pretrained(model_name)\n",
    "        predictions = []\n",
    "        references = []\n",
    "        sessions = ORTModelForSpeechSeq2Seq.load_model(\n",
    "            os.path.join(model_path, 'encoder_model.onnx'),\n",
    "            os.path.join(model_path, 'decoder_model.onnx'),\n",
    "            os.path.join(model_path, 'decoder_with_past_model.onnx'))\n",
    "        self.model = ORTModelForSpeechSeq2Seq(sessions[0], sessions[1], model_config, model_path, sessions[2])\n",
    "        self.forced_decoder_ids = self.processor.get_decoder_prompt_ids(language=\"russian\", task=\"transcribe\")\n",
    "        if  postprocess is not None:\n",
    "            self.post_process =  postprocess\n",
    "        else:\n",
    "            self.post_process = None\n",
    "        self.sampling_rate = 16000\n",
    "    def postprocess(self, output):\n",
    "\n",
    "        output = self.processor.batch_decode(output, skip_special_tokens=True)\n",
    "        return output\n",
    "        \n",
    "    def run(self, speech):\n",
    "        feature = self.processor(speech, sampling_rate=self.sampling_rate, return_tensors=\"pt\")\n",
    "        rez = self.model.generate(feature['input_features'], forced_decoder_ids=self.forced_decoder_ids)\n",
    "        # print(rez.logits[0], self.emotions)\n",
    "        return self.postprocess(rez)\n",
    "\n",
    "sampling_rate =  16000\n",
    "path_a ='01_happiness_anger a_020.wav'\n",
    "speech, sr = librosa.load(path_a, sr=sampling_rate)\n",
    "model_text = SpeechtoText('whisper-tiny_onnx', \"openai/whisper-base\", \"openai/whisper-tiny\")\n",
    "model_text.run(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d2ba8d-ad4a-4e06-9c1c-ae11266ad057",
   "metadata": {},
   "source": [
    "# Речь в эмоции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "add289d5-94c7-4842-b0ba-7aebff4a4e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'angry'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class  SpeechtoEmotion(BaseModel):\n",
    "    def __init__(self, model_name, model_name_preprocess, model_postprocess = None):\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(model_name_preprocess)\n",
    "        self.sampling_rate = self.processor.feature_extractor.sampling_rate\n",
    "        self.model = ORTModelForAudioClassification.from_pretrained(model_name)#\"wav2vec2-xls-r-300m-emotion-ru_onnx\"\n",
    "        self.emotions = ['neutral', 'positive', 'angry', 'sad', 'other']\n",
    "        if  model_postprocess is not None:\n",
    "            self.post_process =  model_postprocess\n",
    "        else:\n",
    "            self.post_process = None\n",
    "    \n",
    "    def postprocess(self, outputs):\n",
    "        return self.emotions[np.argmax(outputs.logits[0])]\n",
    "        \n",
    "    def run(self, speech):\n",
    "        features = self.processor(speech, sampling_rate=self.sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "        rez = self.model(features['input_values'])\n",
    "        # print(rez.logits[0], self.emotions)\n",
    "        return self.postprocess(rez)\n",
    "\n",
    "\n",
    "path_a ='01_happiness_anger a_020.wav'\n",
    "speech, sr = librosa.load(path_a, sr=sampling_rate)\n",
    "model_em = SpeechtoEmotion(\"wav2vec2-xls-r-300m-emotion-ru_onnx\", \"KELONMYOSA/wav2vec2-xls-r-300m-emotion-ru\")\n",
    "model_em.run(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62269d0-a2dd-41c4-9c46-bb24b98d89c1",
   "metadata": {},
   "source": [
    "# Текст в эмоции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3c53929-80f5-477f-b15d-bc863ff37bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sadness'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class  TexttoEmotion(BaseModel):\n",
    "    def __init__(self, model_name, model_name_or_path):\n",
    "        self.processor = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.model = ORTModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.labels = ['neutral', 'joy', 'sadness', 'anger', 'enthusiasm', 'surprise', 'disgust', 'fear', 'guilt', 'shame']\n",
    "        self.labels_ru = ['нейтрально', 'радость', 'грусть', 'гнев', 'интерес', 'удивление', 'отвращение', 'страх', 'вина', 'стыд']\n",
    "    \n",
    "    def postprocess(self, outputs):\n",
    "        return self.labels[np.argmax(outputs.logits[0])]\n",
    "        \n",
    "    def run(self, text):\n",
    "        features = self.processor(text,  max_length=512, truncation=True, return_tensors='pt')\n",
    "        rez = self.model(**features)\n",
    "        # print(features )\n",
    "        return self.postprocess(rez)\n",
    "\n",
    "\n",
    "model_textem = TexttoEmotion(\"rubert-tiny2-russian-emotion-detection_onnx\", \"Djacon/rubert-tiny2-russian-emotion-detection\")\n",
    "model_textem.run(\"ой беда, какая беда\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e02ad7b-cdc7-49c8-b260-8c0f699c3dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You are using a model of type whisper to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Time:  7.873037576675415\n",
      "Result text:  [' Слушай, я потратил обретьом кучу денег для того, чтобы притоситься в эту дару. Это что вообще такое? Посмотри на официантов, они все в черных каких-то рубашках с кислыми минами. Даже никто из них до сих пор не подошел к нам.']\n",
      "Result emotion:  enthusiasm angry\n"
     ]
    }
   ],
   "source": [
    "path_a ='01_happiness_anger a_020.wav'\n",
    "t1 = time.time()\n",
    "speech, sr = librosa.load(path_a, sr=sampling_rate)\n",
    "model_text = SpeechtoText('whisper-tiny_onnx', \"openai/whisper-base\", \"openai/whisper-tiny\")\n",
    "rez_text = model_text.run(speech)\n",
    "\n",
    "model_textem = TexttoEmotion(\"rubert-tiny2-russian-emotion-detection_onnx\", \"Djacon/rubert-tiny2-russian-emotion-detection\")\n",
    "rez_em_text = model_textem.run(rez_text)\n",
    "\n",
    "model_em = SpeechtoEmotion(\"wav2vec2-xls-r-300m-emotion-ru_onnx\", \"KELONMYOSA/wav2vec2-xls-r-300m-emotion-ru\")\n",
    "rez_em_speech = model_em.run(speech)\n",
    "print('Time: ',time.time() - t1)\n",
    "print('Result text: ',rez_text)\n",
    "print('Result emotion: ', rez_em_text, rez_em_speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f196b-ec90-4c1e-b288-9e00a45bdeec",
   "metadata": {},
   "source": [
    "# Конвертация моделей из Hugginface"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85dbe339-1043-4785-8398-fdae40ed905f",
   "metadata": {},
   "source": [
    "Конвертация модели в ONNX\n",
    "\n",
    "1) учтановить optimum-cli (https://github.com/huggingface/optimum)\n",
    "2) для отобранной модели делаем конвертаццию из консоли : \n",
    "optimum-cli export onnx --model <имя модели в Hugginface> <директория куда записать результат конвертации>\n",
    "например для модели KELONMYOSA/wav2vec2-xls-r-300m-emotion-ru запишем ее в каталог wav2vec2-xls-r-300m-emotion-ru_onnx\n",
    "optimum-cli export onnx --model KELONMYOSA/wav2vec2-xls-r-300m-emotion-ru wav2vec2-xls-r-300m-emotion-ru_onnx\n",
    "\n",
    "при подключении используем конструкции типа from optimum.onnxruntime import  ORT<имя задачи для работы>\n",
    "from optimum.onnxruntime import ORTModelForSpeechSeq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6b6471-68a6-4b2e-b754-f25a4e48bc23",
   "metadata": {},
   "source": [
    "# МУСОР"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2cef5681-6b74-4fc4-8101-ff51482ca032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not passing the argument `library_name` to `get_supported_tasks_for_model_type` is deprecated and the support will be removed in a future version of Optimum. Please specify a `library_name`. Defaulting to `\"transformers`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feature-extraction', 'fill-mask', 'text-classification', 'multiple-choice', 'token-classification', 'question-answering']\n"
     ]
    }
   ],
   "source": [
    "from optimum.exporters.tasks import TasksManager\n",
    "distilbert_tasks = list(TasksManager.get_supported_tasks_for_model_type(\"distilbert\", \"onnx\").keys())\n",
    "\n",
    "print(distilbert_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "41dc0261-928f-4dc3-a8ff-5ceff0e8fd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-4.7652, -1.0452, -7.0409, -4.6864, -4.0277, -6.2021, -4.9473,  2.6287,\n",
       "          7.6111, -1.2488, -2.0551, -0.9350,  4.9758, -0.7707,  2.1493, -2.0703,\n",
       "         -4.3232, -4.9472]]), end_logits=tensor([[ 0.4382, -1.6502, -6.3654, -6.0661, -4.1482, -3.5779, -0.0774, -3.6168,\n",
       "         -1.8750, -2.8910,  6.2582,  0.5425, -3.7699,  3.8232, -1.5073,  6.2311,\n",
       "          3.3604, -0.0772]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering, ORTModelForAudioClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert_base_uncased_squad_onnx\")\n",
    "model = ORTModelForQuestionAnswering.from_pretrained(\"distilbert_base_uncased_squad_onnx\")\n",
    "inputs = tokenizer(\"What am I using?\", \"Using DistilBERT with ONNX Runtime!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "21e54d7c-b086-45c7-8152-ae1f6b5f513f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': tensor([[ 0.0044,  0.0131,  0.0104,  ..., -0.0023,  0.0031,  0.0031]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], dtype=torch.int32)}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4f4bf547-98bc-44a5-bdb4-d7345b20103c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00018311,  0.00057983,  0.00045776, ..., -0.00012207,\n",
       "        0.00012207,  0.00012207], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8087a4-5fd6-4d85-9578-632cb1e855b7",
   "metadata": {},
   "source": [
    "### onnx model from : optimum-cli export onnx --model KELONMYOSA/wav2vec2-xls-r-300m-emotion-ru wav2vec2-xls-r-300m-emotion-ru_onnx\n",
    "\n",
    "## from \n",
    "\n",
    "work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c46d831-5773-4d9f-9c6d-1201f40ddb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.0182,  0.0492,  0.0886, -0.0229,  0.0305]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering, ORTModelForAudioClassification\n",
    "from transformers import AutoConfig, Wav2Vec2Processor\n",
    "import librosa\n",
    "\n",
    "path_a ='../dataset/RESD_csv/train/01_happiness_anger/01_happiness_anger a_020.wav'\n",
    "model = ORTModelForAudioClassification.from_pretrained(\"wav2vec2-xls-r-300m-emotion-ru_onnx\")\n",
    "# inputs = tokenizer(\"What am I using?\", \"Using DistilBERT with ONNX Runtime!\", return_tensors=\"pt\")\n",
    "model_name_or_path = \"KELONMYOSA/wav2vec2-xls-r-300m-emotion-ru\"\n",
    "# config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name_or_path)\n",
    "sampling_rate = processor.feature_extractor.sampling_rate\n",
    "speech, sr = librosa.load(path_a, sr=sampling_rate)\n",
    "features = processor(speech, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "outputs = model(features['input_values'])\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f71455ae-5825-40f7-a4cb-38aebb7ca5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Processor:\n",
       "- feature_extractor: Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0,\n",
       "  \"processor_class\": \"Wav2Vec2ProcessorWithLM\",\n",
       "  \"return_attention_mask\": true,\n",
       "  \"sampling_rate\": 16000\n",
       "}\n",
       "\n",
       "- tokenizer: Wav2Vec2CTCTokenizer(name_or_path='KELONMYOSA/wav2vec2-xls-r-300m-emotion-ru', vocab_size=40, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n",
       "\t1: AddedToken(\"<s>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n",
       "\t2: AddedToken(\"</s>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59fee699-a15f-48b7-8342-c5fcb9abb1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization.quantize import quantize\n",
    "from transformers import Wav2Vec2Processor\n",
    "import torch\n",
    "\n",
    "def convert_to_onnx(model_id_or_path, onnx_model_name, path_a = ''):\n",
    "    print(f\"Converting {model_id_or_path} to onnx\")\n",
    "    model = Wav2Vec2Processor.from_pretrained(model_id_or_path).feature_extractor\n",
    "    sampling_rate = model.sampling_rate\n",
    "    speech, sr = librosa.load(path_a, sr=sampling_rate)\n",
    "\n",
    "    x = speech#torch.randn(1, sampling_rate , requires_grad=True)\n",
    "\n",
    "    torch.onnx.export(model,                        # model being run\n",
    "                    x,                              # model input (or a tuple for multiple inputs)\n",
    "                    onnx_model_name,                # where to save the model (can be a file or file-like object)\n",
    "                    export_params=True,             # store the trained parameter weights inside the model file\n",
    "                    opset_version=11,               # the ONNX version to export the model to\n",
    "                    do_constant_folding=True,       # whether to execute constant folding for optimization\n",
    "                    input_names = ['input'],        # the model's input names\n",
    "                    output_names = ['output'],      # the model's output names\n",
    "                    dynamic_axes={'input' : {1 : 'audio_len'},    # variable length axes\n",
    "                                'output' : {1 : 'audio_len'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee587c0-4a74-4463-971a-0591e84a2694",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"KELONMYOSA/wav2vec2-xls-r-300m-emotion-ru\"\n",
    "onnx_model_name = model_name_or_path.split(\"/\")[-1] + \".onnx\"\n",
    "print(onnx_model_name)\n",
    "convert_to_onnx(model_name_or_path, onnx_model_name, path_a ='../dataset/RESD_csv/train/01_happiness_anger/01_happiness_anger a_020.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e3a60ce-e00a-4817-9699-e971c98cd38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601cfadb0e2d4aa492ae6bc1e7beed9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/451 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export the model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913716221a14487fa42339135494c977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcbb359c8b044b39cac0fbf2afdd35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61bb33ed5a2c4b8791f6c98fec57b5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bce50cb4b7a4e93bcddeb1e09411486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "Using framework PyTorch: 2.1.1+cpu\n",
      "/home/boss/.local/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:223: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
     ]
    }
   ],
   "source": [
    "model = ORTModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\", export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d33ddb-bb85-49a0-b888-4a4a3afb9948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering, ORTModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Djacon/rubert-tiny2-russian-emotion-detection\")\n",
    "model = ORTModelForSequenceClassification.from_pretrained(\"rubert-tiny2-russian-emotion-detection_onnx\")\n",
    "inputs = tokenizer(\"ой беда, какая беда\", max_length=512, truncation=True, return_tensors='pt')\n",
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cec21a19-d776-45ed-9c72-71dcab90f0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1391, -2.8492,  1.7666,  0.2717, -3.0771, -2.4219, -0.5103, -1.8227,\n",
       "         -1.0535, -1.0158]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "LABELS = ['neutral', 'joy', 'sadness', 'anger', 'enthusiasm', 'surprise', 'disgust', 'fear', 'guilt', 'shame']\n",
    "LABELS_RU = ['нейтрально', 'радость', 'грусть', 'гнев', 'интерес', 'удивление', 'отвращение', 'страх', 'вина', 'стыд']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Djacon/rubert-tiny2-russian-emotion-detection\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Djacon/rubert-tiny2-russian-emotion-detection\")\n",
    "input = tokenizer(\"ой беда, какая беда\", max_length=512, truncation=True, return_tensors='pt')\n",
    "model(**input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f828740f-3c7c-4fb5-a9da-154e68d887c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a5557ac2-44dc-4084-8951-1db4ef6397fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Слушай, я потратил обретьом кучу денег для того, чтобы притоситься в эту дару. Это что вообще такое? Посмотри на официантов, они все в черных каких-то рубашках с кислыми минами. Даже никто из них до сих пор не подошел к нам.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import Audio, load_dataset\n",
    "\n",
    "# load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"russian\", task=\"transcribe\")\n",
    "\n",
    "# load streaming dataset and read first audio sample\n",
    "# ds = load_dataset(\"common_voice\", \"fr\", split=\"test\", streaming=True)\n",
    "ds = speech\n",
    "sampling_rate=16_000\n",
    "\n",
    "input_speech = speech\n",
    "input_features = processor(speech, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "\n",
    "# generate token ids\n",
    "predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "# decode token ids to text\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad8c9689-1bc7-4dc7-a2d8-6d2ce7755b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSpeechSeq2Seq\n",
    "from datasets import Audio, load_dataset\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "923e10a9-215d-4a89-8cc7-7c5eaeb348bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type whisper to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[50258, 50263, 50359, 50363,  2933, 43689,    11,  2552,  6364, 11157,\n",
       "          2338,  3348,   481,  9108,  1253,   981,  4187,   585, 40957,  5561,\n",
       "         11283,    11,  7887,  1285,  1635,  1885, 12306,   740, 18763,  1070,\n",
       "          2222,   585,    13,  6684,  2143, 14345, 18292,    30, 18689, 44443,\n",
       "          1470, 31950, 30321,  1416,  8642,    11,  7515,  4640,   740, 12360,\n",
       "          5783, 44178,    12,   860, 27371,  6835, 18366,   776,   981, 47105,\n",
       "         24670, 19073,  5150,    13, 42900, 31666,  3943, 14319,  5865,   776,\n",
       "          4165, 11948,  1725,  4095,  6824,  1414,   981, 11401,    13, 50257]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PretrainedConfig\n",
    "model_name = 'openai/whisper-base'\n",
    "model_path = 'whisper-tiny_onnx'\n",
    "model_config = PretrainedConfig.from_pretrained(model_name)\n",
    "predictions = []\n",
    "references = []\n",
    "sessions = ORTModelForSpeechSeq2Seq.load_model(\n",
    "            os.path.join(model_path, 'encoder_model.onnx'),\n",
    "            os.path.join(model_path, 'decoder_model.onnx'),\n",
    "            os.path.join(model_path, 'decoder_with_past_model.onnx'))\n",
    "model = ORTModelForSpeechSeq2Seq(sessions[0], sessions[1], model_config, model_path, sessions[2])\n",
    "model\n",
    "\n",
    "feature = processor(speech, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "model.generate(feature['input_features'], forced_decoder_ids=forced_decoder_ids)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
